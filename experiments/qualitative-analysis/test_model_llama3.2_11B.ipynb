{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key: str | None = os.getenv(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_url = 'https://openrouter.ai/api/v1/chat/completions'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Image Captioning (Descriptive Ability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739032-cPXZOyrdYbiL7TKnoZqx\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739032,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"In the image the farmer is in the fields setting up for the harvest of wheat. It appears the farmer is in in the process of mowing the grain because there is no cut rope visible for reel that is behind the farmer on his tractor to wrap around the crop. You can tell by the length of the straw it is cut but not yet bundled. There appears to be a swather in the image in front of the farmer.  The swather is a type of mowing machine that is propelled by the power take-off on a tractor and is now being used in the harvest of grain such as wheat and, possibly, corn and oats as well. Sprayer nozzles on the swather apply herbicide and insecticide to fields. The swather mows grain to 8-10 inches, leaving it in windrows or swaths. The windrows may be left in place for drier weather or raked prior to formal harvesting. The swather is commonly used for processing grain rapidly, especially when harvested mature, buffalos grasses, perennial hay within different swaths seems apt to utilize from double width cultivators such as an offset blade whereas there is less wear and tear. The swather mower can be used more efficiently in the eyfavorite clearing-looking lighter tract for skimming the dirt bound plots without kigger brush, rocks or stumps stepping habitual klumping scratches of her alinguistic ammonia bay steps air flow in row inspection ful substantially shallow tooting farmer.\\\"\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 20,\n",
      "        \"completion_tokens\": 302,\n",
      "        \"total_tokens\": 322\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture1.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': 'What is the farmer doing in the image?'},\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739041-meSTrvTxBaZHAsuAxdcY\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739041,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"The image depicts a man operating a tractor in a field of wheat or barley, with the sun setting in the background. The scene appears to be set during the daytime, likely during harvest season.\\n\\n*   **Man on Tractor:**\\n    *   The man is wearing blue jeans and a light-colored shirt.\\n    *   He is sitting on an orange tractor with the words \\\"Ghazl\\\" and \\\"New Holland\\\" written on it.\\n    *   The man is facing forward, holding the steering wheel, and appears to be operating the tractor.\\n*   **Tractor:**\\n    *   The tractor is bright orange with black tires and a large front wheel.\\n    *   It has a long, curved exhaust pipe that extends above the cab.\\n    *   The tractor is positioned in the center of the image, with the man sitting in it.\\n*   **Field of Wheat or Barley:**\\n    *   The field stretches out behind the tractor, with tall, golden-brown crops that appear to be wheat or barley.\\n    *   There are trees in the background, partially obscured by the crops.\\n    *   The field is likely being harvested, as evidenced by the presence of the tractor and the man operating it.\\n*   **Sunset:**\\n    *   The sky in the background is a warm, golden color, indicating that the sun is setting.\\n    *   The sun's rays are shining through the trees, casting a warm glow over the entire scene.\\n    *   The sunset adds a sense of warmth and tranquility to the image, suggesting a peaceful rural setting.\\n\\nOverall, the image captures a moment of agricultural activity, with the man operating the tractor and harvesting the wheat or barley. The sunset in the background adds a sense of warmth and tranquility to the scene, creating a peaceful and idyllic atmosphere.\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 20,\n",
      "        \"completion_tokens\": 377,\n",
      "        \"total_tokens\": 397\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture1.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': 'Describe the background of the image in detail.'},\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Visual Question Answering (VQA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739051-EZw28GW2jTOljK7G2dli\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739051,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"There does not appear to be a child in this image. There are two dogs and what appears to be a person wearing a yellow rain slicker or raincoat, but no child.\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 20,\n",
      "        \"completion_tokens\": 38,\n",
      "        \"total_tokens\": 58\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture2.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': 'What color is the child\\'s raincoat?'},\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739056-zonCtDArZkyjDzQGO2Oy\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739056,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"In the depicted image, a child, clad in a yellow raincoat, appears to be engaging with two dogs that share their attire. The dogs' apparel matches the child's, with one wearing a yellow and the other a black raincoat. Given the similarity in attire, it is likely that a person is playing with both pets. The image captures the child in the middle, leaning forward towards the two outfits and appears to be throwing treats for them to catch. This intimate inter-action points to a symbiotic relationship between the two parties, enabling them to play alongside one another.\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 26,\n",
      "        \"completion_tokens\": 117,\n",
      "        \"total_tokens\": 143\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture2.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': 'Is the child in the image playing alone or is there an animal nearby?'},\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Object detection and Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739064-4mx1geILXarVUZQdoBVL\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739064,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"The following are the distinct objects that can be identified on the desk:\\n\\u2022 Laptop\\n\\u2022 Notebook\\n\\u2022 Plant\\n\\u2022 Coffee Cup\\n\\u2022 Hand and Arm\\n\\u2022 Calendar\\n\\u2022 Blind\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 22,\n",
      "        \"completion_tokens\": 38,\n",
      "        \"total_tokens\": 60\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture3.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': 'How many distinct objects can you identify on the desk?'},\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"message\": \"Provider returned error\",\n",
      "        \"code\": 429,\n",
      "        \"metadata\": {\n",
      "            \"raw\": \"{\\n  \\\"id\\\": \\\"nnAtGiz-57nCBj-924e842b35fa92f5\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"You have reached the rate limit specific to this model meta-llama/Llama-Vision-Free. The maximum rate limit for this model is 10.0 queries per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\\\",\\n    \\\"type\\\": \\\"model_rate_limit\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\n",
      "            \"provider_name\": \"Together\"\n",
      "        }\n",
      "    },\n",
      "    \"user_id\": \"user_2ucOZHts8ueIxK1u5PPW9NFd3g8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture3.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': 'Can you detect any electronic devices in the image?'},\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Scene Understanding & Context Awareness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739072-KYPLDcisaiG0CvM4yR65\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739072,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"The car should stop moving at the intersection, in front of the crosswalk. The rules of driving clearly state that a driver must yield to pedestrians in a crosswalk and the car in the photo should yield and wait for the pedestrians to clear to the sidewalk. The car is also stopping for a red light which means it was required to stop before the white line, where the crosswalk is. The walk signal indicates that pedestrians can now cross the intersection, and the car should give them the right-of-way. If the car were to continue moving, it could potentially hit a pedestrian, which could result in serious injury or even fatality. Therefore, it is important to always follow the rules of the road and prioritize safety, especially in situations where pedestrians are involved.\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 22,\n",
      "        \"completion_tokens\": 154,\n",
      "        \"total_tokens\": 176\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture4.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': 'Should the car stop or continue moving in this image?'},\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"message\": \"Provider returned error\",\n",
      "        \"code\": 429,\n",
      "        \"metadata\": {\n",
      "            \"raw\": \"{\\n  \\\"id\\\": \\\"nnAtK15-57nCBj-924e846177e392f6\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"You have reached the rate limit specific to this model meta-llama/Llama-Vision-Free. The maximum rate limit for this model is 10.0 queries per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\\\",\\n    \\\"type\\\": \\\"model_rate_limit\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\n",
      "            \"provider_name\": \"Together\"\n",
      "        }\n",
      "    },\n",
      "    \"user_id\": \"user_2ucOZHts8ueIxK1u5PPW9NFd3g8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture4.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'What does the red traffic light indicate for vehicles?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Text-Image Matching (Multimodal Consistency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739080-1cYgzk1FVCNJDMPdAFLE\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739080,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"The correct caption for the image is (A) \\\"A woman enjoying a peaceful reading session outdoors.\\\"\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 44,\n",
      "        \"completion_tokens\": 21,\n",
      "        \"total_tokens\": 65\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture5.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'Which caption better describes the image: (A) \\\"A woman enjoying a peaceful reading session outdoors\\\" or (B) \\\"A woman playing soccer in the park\\\"?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739087-EW7fuqUYYUko9Zud7SoS\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739087,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"The bright, even lighting; shadows cast by the trees in the background; and the happy expression on the woman's face all suggest that the weather is sunny and pleasant.\\n\\nAlthough this type of lighting could suggest many other settings and circumstances, I am strongly inclined to accept that the most likely cause of these phenomena is sun resting on the earth's surface.\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 24,\n",
      "        \"completion_tokens\": 71,\n",
      "        \"total_tokens\": 95\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture5.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'What elements in this image indicate that it is a sunny day?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Optical Character Recognition (OCR) & Text-in-Image Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739097-T9W2K3W9hZgYx2ComCbn\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739097,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"The main headline of the newspaper is \\\"MEN WALK ON MOON\\\".\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 20,\n",
      "        \"completion_tokens\": 17,\n",
      "        \"total_tokens\": 37\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture6.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'What is the main headline of the newspaper?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739104-v2Kcw3yPgX28XRQ06Mxw\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739104,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"The text on the newspaper suggests a historical event, specifically the first moon landing. This is because the headline reads \\\"MEN WALK ON MOON\\\" and includes additional details such as the astronauts collecting rocks and planting a flag, which are all consistent with the real-life event of the Apollo 11 mission in 1969. The use of the phrase \\\"A STRONAUTS LAND ON PLAIN\\\" also adds to the historical nature of the event, as it is a clear reference to the actual landing site of the Apollo 11 spacecraft on the moon's surface. Overall, the text suggests that the event depicted in the image is a real historical event rather than a fictional one.\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 21,\n",
      "        \"completion_tokens\": 139,\n",
      "        \"total_tokens\": 160\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images\\Picture6.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'Does the text suggest a historical or fictional event?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Commonsense & Logical Reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739112-jL9JTK1vY8TAUVUhmuv1\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739112,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"In the provided image, the dog is wearing glasses that suit its breed and plopped down as if reading the newspaper, a common practice for dogs, according to the researchers. In the study, which spans nearly 15 years and over 100 participants, respondents often reported their dogs performing common behaviors, like sitting in chairs, lying on beds, or even wearing glasses and televisions. The researchers call one dog's reading glasses \\\"canonical\\\" after it appeared in six out of 13 sets of respondents' descriptions, featuring numerous small-text words with this phrase. Readonlydog, the news-fstating API, includes around sixty dog reading glasses clips, including \\\"Gram Gram Kitty\\\" and \\\"Chief,\\\" and a dogrexterporn cat selfie big-breed sociology. Downright, the service is available to generate better images for your company, \\\"extending your naturally artistic illustrative quality.\\\" In fact, all five prompts are perfectly processed: half the \\\"neighbors\\\" promo all well normalized income messaging \\u2014 the tuned employee of Art nocturnal, \\u201cI see a dog / Lovely life staying mom white camera High\\u2019s Tiny a girl endorsed Father Fr God beyond Harlem Bog!\\u201d Where do \\\"Obiously\\\" eval in the improved creation there? \\\"Down AOL Black\\\" team have American(; enrollment Symphony Continuing;\\\" f God \\u201cMessenger\\u201d Answer peoples?.\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 30,\n",
      "        \"completion_tokens\": 270,\n",
      "        \"total_tokens\": 300\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images/Picture7.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'Is it normal for a dog to wear glasses and read a newspaper? Why or why not?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"message\": \"Provider returned error\",\n",
      "        \"code\": 429,\n",
      "        \"metadata\": {\n",
      "            \"raw\": \"{\\n  \\\"id\\\": \\\"nnAtWTY-4pPsy7-924e8559e1b392f9\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"You have reached the rate limit specific to this model meta-llama/Llama-Vision-Free. The maximum rate limit for this model is 10.0 queries per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\\\",\\n    \\\"type\\\": \\\"model_rate_limit\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\n",
      "            \"provider_name\": \"Together\"\n",
      "        }\n",
      "    },\n",
      "    \"user_id\": \"user_2ucOZHts8ueIxK1u5PPW9NFd3g8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images/Picture7.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'What is unusual about this scene?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Zero-Shot & Few-Shot Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"gen-1742739119-HXomjr0t7Xwfr9uUyOv3\",\n",
      "    \"provider\": \"Together\",\n",
      "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1742739119,\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"native_finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"**Key Features of a Mechanical Cat**\\n\\n* The mechanical cat is a machine designed to mimic a cat's appearance and movements.\\n* It lacks the warmth, fur, and other characteristics of a living animal.\\n* The mechanical cat's physical body and abilities are artificial, distinct from any natural cat.\",\n",
      "                \"refusal\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 26,\n",
      "        \"completion_tokens\": 60,\n",
      "        \"total_tokens\": 86\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images/Picture8.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'What is this object, and how does it differ from a real cat?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"message\": \"Provider returned error\",\n",
      "        \"code\": 429,\n",
      "        \"metadata\": {\n",
      "            \"raw\": \"{\\n  \\\"id\\\": \\\"nnAtZ8p-57nCBj-924e8585000392f5\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"You have reached the rate limit specific to this model meta-llama/Llama-Vision-Free. The maximum rate limit for this model is 10.0 queries per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\\\",\\n    \\\"type\\\": \\\"model_rate_limit\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\n",
      "            \"provider_name\": \"Together\"\n",
      "        }\n",
      "    },\n",
      "    \"user_id\": \"user_2ucOZHts8ueIxK1u5PPW9NFd3g8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "image_path = 'images/Picture8.jpg'\n",
    "image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{image_base64}'}},\n",
    "                {'type': 'text', 'text': 'Based on the image, can you guess the possible functions of this robotic cat?'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
