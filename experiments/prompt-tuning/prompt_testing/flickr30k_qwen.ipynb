{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93361d37-7eac-4abb-97f9-faacc6d701b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: We'll be using `/tmp/unsloth_compiled_cache` for temporary Unsloth patches.\n",
      "Standard import failed for UnslothDPOTrainer: No module named 'UnslothDPOTrainer'. Using tempfile instead!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastVisionModel, FastLanguageModel\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoProcessor, TextStreamer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c59bc64-858c-48d1-bc34-f29f04f3edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Qwen2_Vl patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.339 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99afcd6c1a344beaefef5f76338b7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9391d2ce28fa42628c65e07a4d79759c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99eee8cce56a4a87aed18ebec5d0ed80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/572 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60e3cf6ccb44e5183873bd0d2905d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669c215de3b04a67833b20dcefc01b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe04cb3159b44bb8bf8b637dcd43d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768aa6504d4944e4a9a262ff175ae7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5843dbd0d6124c10acdf8128685515de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42ebfa1c34549da918024d825a4805c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0262749479d143c2ba457f1336654ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decc91c6-c815-4ec1-8939-2f7fb22ffa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d19a4dba8f48e996780d8f5d3f80ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/572 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0778386eac4d6a800e816cfff82417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f833b20c5e684233b293ab1dcdada040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2e805729c94790b3bf8d3b1fbcf2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef7ffb04d814d87a002d91f42edb509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde2f9fc67ef495b9146c00b7948263c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00d5f9ddd2545f892f19d7926d3507a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ac8f6b8f614ebeb1f24ee66aa37141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"unsloth/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "model = FastVisionModel.for_inference(model)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac930dd8-f59b-4361-bae0-c39f77143b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk(\"/workspace/data/filtered_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd2b1d-955f-4085-827e-1e0a20c7f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtered test Dataset:\")\n",
    "print(test_dataset)\n",
    "\n",
    "print(f\"Loaded {len(test_dataset)} image-caption entries\\n\")\n",
    "\n",
    "print(\"📌 First 5 entries:\")\n",
    "first_5 = test_dataset.select(range(5))\n",
    "for row in first_5:\n",
    "    print(row, \"\\n\")\n",
    "\n",
    "print(\"📋 Column types:\")\n",
    "print(test_dataset.features)\n",
    "\n",
    "print(\"\\n🔎 Size\")\n",
    "print((len(test_dataset), len(test_dataset.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e0a55-55b5-4388-a1df-333e034d2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "filename_counts = Counter(test_dataset[\"filename\"])\n",
    "\n",
    "# Print filenames with more than 1 occurrence\n",
    "for filename, count in filename_counts.items():\n",
    "    if count > 1:\n",
    "        print(f\"{filename}: {count} occurrences\")\n",
    "\n",
    "num_duplicates = sum(1 for count in filename_counts.values() if count > 1)\n",
    "print(f\"\\nTotal filenames with duplicates: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963d366-f218-4c2a-9718-3498070a1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import TextStreamer\n",
    "import pandas as pd\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "def run_vlm_inference(prompt: str, filename: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform inference on a given image (by filename) from the dataframe using a custom prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt text (can include mask tokens)\n",
    "        filename (str): Filename of the image in the DataFrame\n",
    "        df (pd.DataFrame): DataFrame with 'filename' and 'image' columns\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, float, float]: (Generated output, inference time in seconds, VRAM used in GB)\n",
    "    \"\"\"\n",
    "    row = df[df[\"filename\"] == filename]\n",
    "    if row.empty:\n",
    "        print(f\"[ERROR] No image found with filename: {filename}\")\n",
    "        return None, 0.0, 0.0\n",
    "\n",
    "    row = row.iloc[0]\n",
    "    image = row[\"image\"]\n",
    "    caption = row[\"caption\"]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image\", \"image\": image}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(images=image, text=input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_mem = torch.cuda.memory_allocated() / 1024 / 1024 / 1024  # in GB\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"🔹 Image: {filename}\")\n",
    "    print(f\"🧾 Prompt: {prompt}\")\n",
    "    print(\"📤 Output:\")\n",
    "\n",
    "    # Perform inference\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=64,\n",
    "        use_cache=True,\n",
    "        temperature=1.5,\n",
    "        min_p=0.1,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_mem = torch.cuda.max_memory_allocated() / 1024**3\n",
    "\n",
    "    time_taken = round(end_time - start_time, 3)\n",
    "    vram_used = round(end_mem - start_mem, 3)\n",
    "    decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"⏱️ Time taken: {time_taken} sec | 🧠 VRAM used: {vram_used} GB\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return decoded_output, time_taken, vram_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbeb36-49d2-4931-95d9-6a8c14f39085",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_set_0 = [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a nature trail guide. Describe what’s happening in this scene to someone preparing for their first family hike.\",\n",
    "               \"There is a <text_1> walking on a <text_2> in this image.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_1= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a food critic observing customer service at cafes. Describe the interaction happening in this scene.\",\n",
    "               \"A <text_1> is standing beside a <text_2> who is sitting down at a table.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_2= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a weather reporter documenting children’s routines during rainy mornings. Describe what’s going on in this image.\",\n",
    "               \"<text_1> are walking with <text_2> in the rain.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_3= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a journalist covering a small-town parade. Describe the role and setting of the clown in this festive scene.\",\n",
    "               \"A <text_1> with bright makeup and colorful clothes stands near a <text_2> during an event.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_4= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a winter sports photographer recounting a joyful moment you captured. Describe the scene with vivid detail.\",\n",
    "               \"Two <text_1> are sitting on a snowy slope with their arms <text_2>.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_5= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a market researcher documenting traditional grocery stores. Describe what the boy is doing and what the store looks like.\",\n",
    "               \"A <text_1> is behind a counter filled with <text_2> in a small shop.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_6= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are an IT workplace culture analyst. Describe the work environment and team dynamic shown in this image.\",\n",
    "               \"A <text_1> is using a laptop at a desk while two <text_2> are in the background.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_7= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a canine trainer assessing a working dog’s field behavior. Describe the dog’s posture and role based on the scene..\",\n",
    "               \"A <text_1> is wearing a vest and holding a <text_2> in its mouth.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_8= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a cultural anthropologist documenting traditional street labor. Describe what the person is doing and how they are carrying the items.\",\n",
    "               \"A <text_1> is walking while carrying <text_2> suspended from a pole.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "prompt_set_9= [\"\",\n",
    "              \"How many people are playing soccer in this scene?\",\n",
    "               \"An image of...\",\n",
    "               \"You are a museum docent giving a tour. Describe the activity of the woman in the context of the art gallery.\",\n",
    "               \"A <text_1> is painting a replica of an artwork in front of <text_2>.\",\n",
    "               \"Describe using format - Subject:  \\_\\_\\_; Activity:  \\_\\_\\_; Environment:  \\_\\_\\_; Additional Notes: \\_\\_\\_;\"\n",
    "               ]\n",
    "PROMPT_LIST = [prompt_set_0, prompt_set_1, prompt_set_2, prompt_set_3, prompt_set_4, prompt_set_5, prompt_set_6, prompt_set_7, prompt_set_8, prompt_set_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc920a-df97-44cf-b834-bee3c5943a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_ids = [2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509]\n",
    "\n",
    "all_outputs = {}\n",
    "all_times = {}\n",
    "all_vram = {}\n",
    "\n",
    "# Loop through each image and its corresponding prompt set\n",
    "for i, img_id in enumerate(test_img_ids):\n",
    "    row = test_dataset.filter(lambda x: x[\"img_id\"] == str(img_id))\n",
    "    if len(row) == 0:\n",
    "        print(f\"[WARNING] No image found with img_id {img_id}\")\n",
    "        continue\n",
    "\n",
    "    filename = row[0][\"filename\"]\n",
    "    prompts = PROMPT_LIST[i]\n",
    "\n",
    "    print(f\"\\n🔍 Running prompts for image: {filename} (img_id: {img_id})\\n\" + \"-\"*60)\n",
    "\n",
    "    inference_outputs = []\n",
    "    inference_times = []\n",
    "    vram_usages = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        output, time_taken, vram_used = run_vlm_inference(prompt, filename, df=test_dataset)\n",
    "        inference_outputs.append(output)\n",
    "        inference_times.append(time_taken)\n",
    "        vram_usages.append(vram_used)\n",
    "\n",
    "    # Save to overall dicts\n",
    "    all_outputs[img_id] = inference_outputs\n",
    "    all_times[img_id] = inference_times\n",
    "    all_vram[img_id] = vram_usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713f9a7-3465-4824-9c7e-ac0fe3e9a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_id in all_outputs:\n",
    "    print(f\"\\n🖼️ Image ID: {img_id}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (output, time_taken, vram_used) in enumerate(zip(all_outputs[img_id], all_times[img_id], all_vram[img_id])):\n",
    "        print(f\"🔹 Prompt {i+1}\")\n",
    "        print(f\"📝 Output: {output}\")\n",
    "        print(f\"⏱️ Inference Time: {time_taken:.3f} sec\")\n",
    "        print(f\"💾 VRAM Used: {vram_used:.3f} GB\")\n",
    "        print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
