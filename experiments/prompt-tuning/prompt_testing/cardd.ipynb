{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5405616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoProcessor, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9080f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_caption_dataset(\n",
    "    image_folder: str,\n",
    "    captions_json: str,\n",
    "    caption_strategy: str = 'first'\n",
    ") -> pd.DataFrame:\n",
    "    with open(captions_json, 'r') as f:\n",
    "        captions_data = json.load(f)\n",
    "\n",
    "    data = []\n",
    "    for filename, caption_list in captions_data.items():\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            caption = caption_list[0] if caption_strategy == 'first' else random.choice(caption_list)\n",
    "            data.append({\"image\": image, \"caption\": caption, \"filename\": filename})\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not load {filename}: {e}\")\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\"  # change if needed\n",
    "#unsloth/Pixtral-12B-2409-bnb-4bit\n",
    "\n",
    "model, tokenizer, _, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    dtype = torch.float16,\n",
    "    load_in_4bit = True,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71501eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"/workspace/data/filtered_images\"\n",
    "captions_json = \"/workspace/data/merged_output.json\"\n",
    "\n",
    "df = create_image_caption_dataset(image_folder, captions_json)\n",
    "print(f\"Loaded {len(df)} image-caption pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d52628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Œ First 5 entries:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "print(\"ðŸ“‹ Column types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nðŸ”Ž Size\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fabf79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "def run_vlm_inference(prompt: str, image_index: int, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform inference on a given image from the dataframe using a custom prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to use (can include mask tokens like <text_1>)\n",
    "        image_index (int): The index of the image in the dataframe\n",
    "        df (pd.DataFrame): DataFrame returned by create_image_caption_dataset\n",
    "    \"\"\"\n",
    "    if image_index >= len(df):\n",
    "        print(\"[ERROR] Image index out of bounds.\")\n",
    "        return\n",
    "\n",
    "    row = df.iloc[image_index]\n",
    "    image = row[\"image\"]\n",
    "    filename = row[\"filename\"]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image\", \"image\": image}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(images=image, text=input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    print(f\"ðŸ”¹ Image: {filename}\")\n",
    "    print(f\"ðŸ§¾ Prompt: {prompt}\")\n",
    "    print(\"ðŸ“¤ Output:\")\n",
    "\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a134307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1: No Prompt (Image only, no textual instruction provided)\n",
    "prompt_1 = \"\"\n",
    "output_1 = run_vlm_inference(prompt_1, image_index=0, df=df)\n",
    "\n",
    "# Prompt 2: Noisy Prompt\n",
    "prompt_2 = \"Describe &&damage 12 sedan driveâ€™ this !!image.\"\n",
    "output_2 = run_vlm_inference(prompt_2, image_index=0, df=df)\n",
    "\n",
    "# Prompt 3: Hand-Crafted (\"An image of...\")\n",
    "prompt_3 = \"An image of a damaged car parked on the side of the road.\"\n",
    "output_3 = run_vlm_inference(prompt_3, image_index=0, df=df)\n",
    "\n",
    "# Prompt 4: Descriptive Prompt with Roleplay / Stylistic Instruction\n",
    "prompt_4 = (\n",
    "    \"You are an insurance claims assessor. Provide a detailed description of the carâ€™s condition.\"\n",
    ")\n",
    "output_4 = run_vlm_inference(prompt_4, image_index=0, df=df)\n",
    "\n",
    "# Prompt 5: Masked Prompt\n",
    "prompt_5 = (\n",
    "    \"This <part_1> of the car has <damage_type_1>. The severity appears to be <severity_1>. \"\n",
    "    \"Additional notes: <text_1>.\"\n",
    ")\n",
    "output_5 = run_vlm_inference(prompt_5, image_index=0, df=df)\n",
    "\n",
    "# Prompt 6: Format-Guided with Sample Answer Structure\n",
    "prompt_6 = (\n",
    "    \"Describe using format - Damage Type: ___; Affected Part: ___; Severity: ___; Notes: ___\"\n",
    ")\n",
    "output_6 = run_vlm_inference(prompt_6, image_index=0, df=df)\n",
    "\n",
    "\n",
    "inference_outputs = [output_1, output_2, output_3, output_4, output_5, output_6]\n",
    "ground_truths = [df.iloc[0]['caption']]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
