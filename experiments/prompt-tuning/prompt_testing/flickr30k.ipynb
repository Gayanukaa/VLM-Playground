{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93361d37-7eac-4abb-97f9-faacc6d701b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a501ae-ca91-4774-845d-82f33f8f7125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered test Dataset:\n",
      "Dataset({\n",
      "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "with open('/workspace/data/dataset_flickr30k.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "image_entries = json_data['images']\n",
    "test_filenames = [image['filename'] for image in image_entries if image['split'] == 'test'][:50]\n",
    "\n",
    "flickr30k_test = load_dataset(\"nlphuji/flickr30k\", split=\"test\")\n",
    "test_dataset = flickr30k_test.filter(lambda example: example['filename'] in test_filenames)\n",
    "\n",
    "print(\"Filtered test Dataset:\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a16384b-09ed-4c69-9045-86ef58ebe73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 image-caption entries\n",
      "\n",
      "üìå First 5 entries:\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x461 at 0x7FECFD27A530>, 'caption': ['The man with pierced ears is wearing glasses and an orange hat.', 'A man with glasses is wearing a beer can crocheted hat.', 'A man with gauges and glasses is wearing a Blitz hat.', 'A man in an orange hat starring at something.', 'A man wears an orange hat and glasses.'], 'sentids': ['125', '126', '127', '128', '129'], 'split': 'test', 'img_id': '25', 'filename': '1007129816.jpg'} \n",
      "\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x333 at 0x7FECFD279FC0>, 'caption': ['A black and white dog is running in a grassy garden surrounded by a white fence.', 'A Boston Terrier is running on lush green grass in front of a white fence.', 'A black and white dog is running through the grass.', 'A dog runs on the green grass near a wooden fence.', 'A Boston terrier is running in the grass.'], 'sentids': ['170', '171', '172', '173', '174'], 'split': 'test', 'img_id': '34', 'filename': '1009434119.jpg'} \n",
      "\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=347x500 at 0x7FECFD278F40>, 'caption': ['A young female student performing a downward kick to break a board held by her Karate instructor.', 'Girl about to kick a piece of wood in half while karate instructor holds it', 'A girl kicking a stick that a man is holding in tae kwon do class.', 'A girl in karate uniform breaking a stick with a front kick.', 'A girl breaking boards by using karate.'], 'sentids': ['255', '256', '257', '258', '259'], 'split': 'test', 'img_id': '51', 'filename': '101362133.jpg'} \n",
      "\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x7FECFD27A260>, 'caption': ['Five snowmobile riders all wearing helmets and goggles line up in a snowy clearing in a forest in front of their snowmobiles; they are all wearing black snow pants and from left to right they are wearing a black coat, white coat, red coat, blue coat, and black coat.', 'Five people wearing winter jackets and helmets stand in the snow, with snowmobiles in the background.', 'Five people wearing winter clothing, helmets, and ski goggles stand outside in the snow.', 'A group of snowmobile riders gather in the snow.', 'Group gathered to go snowmobiling.'], 'sentids': ['445', '446', '447', '448', '449'], 'split': 'test', 'img_id': '89', 'filename': '102617084.jpg'} \n",
      "\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x7FECFD27A2F0>, 'caption': ['Two men sitting on the roof of a house while another one stands on a ladder.', 'Two men on a rooftop while another man stands atop a ladder watching them', 'Three men, one on a ladder, work on a roof.', 'People are fixing the roof of a house.', 'Three men are working on a roof.'], 'sentids': ['485', '486', '487', '488', '489'], 'split': 'test', 'img_id': '97', 'filename': '10287332.jpg'} \n",
      "\n",
      "üìã Column types:\n",
      "{'image': Image(mode=None, decode=True, id=None), 'caption': [Value(dtype='string', id=None)], 'sentids': [Value(dtype='string', id=None)], 'split': Value(dtype='string', id=None), 'img_id': Value(dtype='string', id=None), 'filename': Value(dtype='string', id=None)}\n",
      "\n",
      "üîé Size\n",
      "(50, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(test_dataset)} image-caption entries\\n\")\n",
    "\n",
    "print(\"üìå First 5 entries:\")\n",
    "first_5 = test_dataset.select(range(5))\n",
    "for row in first_5:\n",
    "    print(row, \"\\n\")\n",
    "\n",
    "print(\"üìã Column types:\")\n",
    "print(test_dataset.features)\n",
    "\n",
    "print(\"\\nüîé Size\")\n",
    "print((len(test_dataset), len(test_dataset.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784e0a55-55b5-4388-a1df-333e034d2b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total filenames with duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "filename_counts = Counter(test_dataset[\"filename\"])\n",
    "\n",
    "# Print filenames with more than 1 occurrence\n",
    "for filename, count in filename_counts.items():\n",
    "    if count > 1:\n",
    "        print(f\"{filename}: {count} occurrences\")\n",
    "\n",
    "num_duplicates = sum(1 for count in filename_counts.values() if count > 1)\n",
    "print(f\"\\nTotal filenames with duplicates: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c59bc64-858c-48d1-bc34-f29f04f3edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoModelForVision2Seq\n",
    "import torch\n",
    "\n",
    "model_id = \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\"\n",
    "#unsloth/Pixtral-12B-2409-bnb-4bit\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    dtype = torch.float16,\n",
    "    load_in_4bit = True,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c963d366-f218-4c2a-9718-3498070a1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import TextStreamer\n",
    "import pandas as pd\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "def run_vlm_inference(prompt: str, filename: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform inference on a given image (by filename) from the dataframe using a custom prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt text (can include mask tokens)\n",
    "        filename (str): Filename of the image in the DataFrame\n",
    "        df (pd.DataFrame): DataFrame with 'filename' and 'image' columns\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, float, float]: (Generated output, inference time in seconds, VRAM used in GB)\n",
    "    \"\"\"\n",
    "    row = df[df[\"filename\"] == filename]\n",
    "    if row.empty:\n",
    "        print(f\"[ERROR] No image found with filename: {filename}\")\n",
    "        return None, 0.0, 0.0\n",
    "\n",
    "    row = row.iloc[0]\n",
    "    image = row[\"image\"]\n",
    "    caption = row[\"caption\"]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image\", \"image\": image}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(images=image, text=input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_mem = torch.cuda.memory_allocated() / 1024 / 1024 / 1024  # in GB\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"üîπ Image: {filename}\")\n",
    "    print(f\"üßæ Prompt: {prompt}\")\n",
    "    print(\"üì§ Output:\")\n",
    "    \n",
    "    # Perform inference\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_mem = torch.cuda.max_memory_allocated() / 1024**3\n",
    "\n",
    "    time_taken = round(end_time - start_time, 3)\n",
    "    vram_used = round(end_mem - start_mem, 3)\n",
    "    decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"‚è±Ô∏è Time taken: {time_taken} sec | üß† VRAM used: {vram_used} GB\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return decoded_output, time_taken, vram_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfae92f-06e7-499c-b0be-09097616e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"000404.jpg\"\n",
    "\n",
    "row = df[df[\"filename\"] == filename]\n",
    "ground_truth = row.iloc[0][\"caption\"]\n",
    "\n",
    "inference_outputs = []\n",
    "vram_usages = []\n",
    "inference_times = []\n",
    "\n",
    "prompts = [\n",
    "    \"\",  # No Prompt\n",
    "    \"Describe &&damage 12 sedan drive‚Äô this !!image.\",  # Noisy\n",
    "    \"An image of a damaged car parked on the side of the road.\",  # Hand-crafted\n",
    "    \"You are an insurance claims assessor. Provide a detailed description of the car‚Äôs condition.\",  # Roleplay\n",
    "    \"This <part_1> of the car has <damage_type_1>. The severity appears to be <severity_1>. Additional notes: <text_1>.\",  # Masked\n",
    "    \"Describe using format - Damage Type: ___; Affected Part: ___; Severity: ___; Notes: ___\"  # Format-Guided\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    output, time_taken, vram = run_vlm_inference(prompt, filename, df=df)\n",
    "    inference_outputs.append(output)\n",
    "    inference_times.append(time_taken)\n",
    "    vram_usages.append(vram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
